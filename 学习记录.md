### 在没有探索情形下的离策略强化学习
1. 如果代理在固定数据集上学习，已经有很多实用算法。由于推断产生的误差，标准的DQN或者DDPG算法无法处理在缺乏数据对当前分布进行矫正情形下的学习问题，因此在处理这类固定数据集问题的时候乏力。作者提出了新的离策略学习算法，限制了动作空间，迫使代理朝着当前数据集给出的在策略的方向移动。
2. 固定数据集的学习的使用场景为：数据的收集过程耗时、耗财、有风险。在模仿学习中，如果学习基于次优的轨迹进行训练，那么多数模仿学习算法都会失效，或者此时需要与环境有更多的交互。
3. 批强化学习提供了一个对于数据质量无要求的学习方法，也即数据分布可以提供对真实分布的有偏逼近。
4. 但目前为止大多数离策略强化学习算法都是增长批次的学习方法，但如果在当前策略下的数据集是对于真实分布的有偏差的估计，那么当前主流的离策略算法都会失效。作者指出，离策略代理在根据相同数据集用相同算法进行训练时，可以表现得很差。
5. 作者认为学习真实分布的不稳定来源的问题为"推断误差"：也即对未被观测到的状态-动作对的错误估计。推断误差可能导致策略产生的数据分布和包含在批次数据中分布的错配。为了克服这个问题，作者引入了限制批强化学习，在极大化回报的同时最小化当前策略产生的状态-动作对和包含在数据集中的状态动作对之间的错配误差。作者证明了BCQ，batch-constrained deep Q-learning 对于从不完备数据集中产生无偏价值函数估计。
6. 通过考虑推断误差，BCQ 可以在不与环境交互的情形下成功学习。
7. 误差主要分为三部分：1. 数据量不够带来的误差；2. 模型本身的误差，主要由于逼近Bellman算子带来的误差；3. 训练时的错配；
8. 在离策略的情形之下，推断误差可能永远得不到矫正

### 带有经验回放的采样有效(sample efficient)的 AC 算法( ACER )
1. 考虑到很多环境中进行采样耗费时间，代价巨大，为了减少采样时的支出，因此我们需要进行有效采样。经验回放是一个有效途径。基于经验回放的离策略学习方法
是一个易于想到的方法。
2. DQN有两个重大限制：首先，最优策略的确定性本质限制了在对抗领域的使用；其次，对于大的动作空间来说找到相对于Q函数的贪心策略比较困难。
3. A3C不是采样有效的方法，但是适用于连续空间和离散空间。
4. ACER 结合了缩减方差技巧、离策略retrace算法、并行训练、用偏差矫正来作截断重要性采样、随机竞争网络结构和有效信赖域策略优化，并且证明了
retrace算子可以被重写为带有误差矫正的重要性采样
5. A3C的策略梯度估计子结合了k步回报和函数逼近，这是综合考虑了方差和偏差的结果；k步回报$R_{t}$的方差很高，但是偏差很低，函数逼近子的偏差很高，但是方差低。
6. ACER使用一个单独的神经网络来估计策略和价值函数，大多数参数是共享的。
7. 重要性采样是最受欢迎(最简单)的离策略学习方法，带有重要性权重的策略梯度可以表述为这些重要性权重的乘积乘上策略梯度逼近子。由于是k个数连乘积的形式，
因此虽然是无偏估计，却有着高方差。为了防止发散，通常会对这一乘积作出截断；尽管截断之后在方差上有上界了，但是却承受着有偏估计的代价。因此需要估计边际
重要性权重。
8. 如何估计梯度逼近子中的价值函数$Q^{\pi}$
9. ACER中采用Retrace算法来估计$Q^{\pi}$，Retrace是一类离策略、基于回报的算法，而且方差低，并且在表格的情形下被证明收敛到目标策略的价值函数。Retrace通过递归式的形式定义。由于Retrace使用了多步回报，因此在策略梯度的估计中会显著减小偏差。
10. 在学习“评论家”$Q_{\theta}(x_{t}, a_{t})$的过程中，再次使用Retrace作为均方误差中的目标然后更新参数。由于Retrace是基于汇报的，因此对于“评论家”的学习也会更快。因此使用Retrace会带来两重好处，第一是减小了策略梯度估计的方差，其次是降低了偏差。


### Attention that does not Explain Away
1. 从概率中高斯混合模型的角度理解 Transformer 架构。提出了双正则化的注意力格式：易于实现，且能提供理论保证，避免了"explain away"效应
2. 注意力机制允许输入之间的信息在任意距离之间传递，适合于在输入之间存在模式交叉的任务。
3. 将Transformer结构视为从低层到高层的数据流，高层为高斯混合模型的输出。作者定义了"explain away"效应：也就是在低层被表示的信息有可能被完全过滤掉。对于高斯混合过程来说，也不是所有的高斯中心(低层神经元)都会生成输出数据，也就是高层神经元。这个效应和有向图模型有关联，
4. 原本的Transformer机制只需要对于每一个上层神经元正则化一次权重，但是作者提出的机制做了两次权重的正则化。更新两次的过程正好对应到约束优化问题中的 Sinkhorn 算法。作者还提出了一种混合注意力方法。


### Deep Residual Learning for Image Recognition
1. 深度残差网络比一般卷积神经网络能达到更多的层数，文章提供了可理解的经验证据表明残差网络比一般网络要易于优化，并且从加深的网络层中得到了精确性的提升。对于很多可视化任务来说，表示的深度很重要。
2. 一堆大道理的说明：深度网络集成了低中高层级的特征，而且特征的层级可以通过堆叠的层数来丰富。但是深层网络的一个问题是梯度消失或者梯度爆炸，这些可以通过正则初始化或者中间的归一化层得以缓解。随着网络开始收敛，会伴随着一些新问题的出现，准确度开始饱和，接着开始衰退，但这种衰退也不是由于过拟合引起的。
3. 存在一个构建深度模型的方法：添加一个恒等映射层。恒等映射在实现的时候可以通过短路连接实现，也就是跳过某些层直接连接到后面的层。
4. 文章在ImageNet数据集上使用了可以理解的实验来表明饱和问题并且评估自己的方法。并且表明：深层残差网络很容易优化，但是一般的深层网络会随着深度的增加误差变大。而且作者提出的深层网络可以很容易从大量增长的层数中得到精确性的提升。


### 指针网络
1. 作者引入了一个新的构造：来学习对于给定输入序列返回其对应位置的条件概率。这种类型的问题无法通过目前的s2s机制或者神经图灵机解决，因为每一步中输出的类别目标数取决于输入的长度，是一个变量。作者通过使用提出的神经注意力的机制来解决这个问题。
2. 新引入的s2s机制移除了长度限制，通过使用一个RNN将输入序列映射到嵌入序列，另外有一个RNN将嵌入映射到一个输出序列。通过输入额外的内容信息增广decoder，从基于内容的注意力机制。但是这些方法依然有限制，需要输出字典的维度预先固定，因此这一方法无法应用到输出长度依赖于输入的变长字典。因此作者通过注意力机制创建了指向输入元素的指针来消除这一限制，因此得到被作者称之为指针网络的架构。
3. 作者在文中作出的主要贡献为：
    - 提出了成为指针网络的新架构，简单而且有效，通过使用softmax概率作为指针来表示变长字典。
    - 应用指针网络解决 3 类迥然不同的问题，包括几何问题和算法问题
    - 数据驱动的算法可以近似解决NP难的问题

4. s2s机制和注意力机制是指针网络的基础。其大致内容可叙述如下。


### High Acceleration Reinforcement Learning for Real-World Juggling with Binary Rewards
1. 对于机器人系统来说在不损害系统的情况下实施动作这一点很重要；因此放大样本效率和安全性对机器人学习算法来说很有必要性；作者没有考虑从设计算法的角度出发，而是提出了一个新系统，直接将这些设置耦合进策略表示、初始化还有最优化的设计中。在二元奖励信号的引导之下，这个系统可以实现杂耍。
2. 多数算法忽略了现实的复杂性，现实中通常需要实时、单例；因此，学习过程必须不能使用不平稳的动作来损害机器人，且必须是采样有效的。最优策略依赖于确切的机器人实例。而且必须按照个体进行学习。阐述了高加速比任务的必要性
3. 本文关注更多的是机器人系统工程实现的细节。采用了二元奖励而不是频繁奖励。基于模型的算法会比无模型算法更为采样有效。


## 多智能体学习相关专题

### LOLA(learning with opponent learning awareness) 对手感知学习
1. 多智能体的设定会使得训练问题变为非稳态问题，因此会使得训练不稳定或者出现意想不到的结果。在LOLA中，作者每一个智能体都会改变其它智能体的预期学习，LOLA的学习规则包含进来了额外的一项，其中考虑了一个智能体对于其它智能体参数更新时的影响，LOLA智能体可以收敛到纳什均衡。通过显式考虑进来了其他智能体的学习过程，LOLA学会了合作。
2. 多智能体强化学习通常考虑了全面合作的设定。通常来说 MA 的弱点在于它们无法考虑其他智能体的学习过程，仅仅简单将对手看作静态环境的一部分。
3. LOLA考虑进来了社交环境中其他智能体的推理行为，同时考虑进来了一个智能体的策略对于其他智能体学习过程的影响。证明了能导致紧急互惠。。。
4. 作者证明了LOLA会导致高的社交互惠，作者还拓展了LOLA到其他设定中，其中对手的策略是未知的，需要从对对手行为的观测中进行推断。


### Learning to Communicate with Deep Multi-Agent Reinforcement Learning
1. 所考虑的模型是完全合作的、部分可观测的、多智能体序列决策问题。这些智能体的目标在于尽量增大相同的折扣奖励。每个智能体都无法观测到Markov转移态，每个智能体都会收到和每个态相关的私有的观测。每个智能体除了可以采取影响环境的动作之外，还能与其同伴进行沟通交流，智能体必须发现一个通信协议以使其能够解决定位自己的行为，并且解决这个任务。
2. 作者关注于中心化学习但是分散化执行的设定，也就是说，智能体之间的交流在学习期间不受限制，是由一个中心化算法实施的，但是在执行中心策略时，智能体仅能通过受限带宽的通道进行，譬如在仿真环境中进行训练的机器人团体可以通过这样的算法得到解决。中心化决策、分散化执行也是MA规划中的标准范式。
3. 作者形式化了两种方式，第一种强化智能体内学习，使用深度Q学习以及RNN来解决部分可观测的问题，在这种方法的一个变式 -- 独立Q学习中。另一种方法，可微内智能体学习，基于一个观察:中心化学习的策略也比参数共享可以提供更多机会。

### QMIX全文
1. 




set nocompatible
set number 
set cursorline
set ruler 
set shiftwidth=4
set softtabstop=4
set tabstop=4
set nobackup
set autochdir

set encoding=utf-8
set termencoding=utf-8
